{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Azure Machine Learning YOLOX Custom Environment\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace - [Configure workspace](../../jobs/configuration.ipynb) \n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../README.md) - check the getting started section\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Create a custom environment from python SDK using\n",
    "  - A docker context\n",
    "\n",
    "**Motivations** - Azure Machine Learning environments are an encapsulation of the environment where your machine learning training happens. By default your workspace has several curated environments already available. This notebook explains how to create a custom environment to run your specific task if you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "msdoc": "how-to-manage-environments-v2.md",
    "name": "libraries",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Environment\n",
    "Azure Machine Learning [environments](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments) are an encapsulation of the environment where your machine learning training happens. They specify the Python packages, environment variables, and software settings around your training and scoring scripts. They also specify run times (Python, Spark, or Docker). The environments are managed and versioned entities within your Machine Learning workspace that enable reproducible, auditable, and portable machine learning workflows across a variety of computes.\n",
    "\n",
    "The workspace contains several curated environments by default to use as-is. However, you can create your own custom environment to meet your specific needs.\n",
    "\n",
    "The `Environment` class will be used to create a custom environment. It accepts the following key parameters:\n",
    "- `name` - Name of the environment.\t\t\n",
    "- `version`\t- Version of the environment. If omitted, Azure ML will autogenerate a version.\t\t\n",
    "- `image` - The Docker image to use for the environment. Either `image` or `build` is required to create environment.\n",
    "- `conda_file` - The standard conda YAML [configuration file](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-file-manually) of the dependencies for a conda environment. It can be used with a `image`. If specified, Azure ML will build the conda environment on top of the Docker image provided.\n",
    "- `BuildContext`- The Docker build context configuration to use for the environment. Either `image` or `build` is required to create environment.\n",
    "  - `path`- Local path to the directory to use as the build context.\t\t\n",
    "  - `dockerfile_path` - Relative path to the Dockerfile within the build context.\n",
    "- `description`\t- Description of the environment.\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create a custom environment from Docker build context configuration\n",
    "In this sample we will use a local docker build configuration to create an environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p YOLOX-docker-contexts/python-and-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./YOLOX-docker-contexts/python-and-pip/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./YOLOX-docker-contexts/python-and-pip/Dockerfile\n",
    "\n",
    "FROM mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1:18\n",
    "\n",
    "RUN git clone https://github.com/Megvii-BaseDetection/YOLOX\n",
    "WORKDIR \"YOLOX\"\n",
    "\n",
    "# python installs\n",
    "RUN pip3 install -r requirements.txt \n",
    "RUN pip3 install -v -e .\n",
    "\n",
    "WORKDIR \"/\"\n",
    "\n",
    "# set command\n",
    "CMD [\"bash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "msdoc": "how-to-manage-environments-v2.md",
    "name": "create_from_docker_context",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading python-and-pip (0.0 MBs): 100%|██████████| 269/269 [00:00<00:00, 9066.03it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'YOLOX-docker-context-example', 'description': 'YOLOX Environment created from a Docker context.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourceGroups/rg-dp100-l/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-l/environments/YOLOX-docker-context-example/versions/4', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/my-compute-instance-a1/code/azureml-examples/sdk/python/assets/environment', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fb3ba628b80>, 'serialize': <msrest.serialization.Serializer object at 0x7fb3ba629240>, 'version': '4', 'conda_file': None, 'build': <azure.ai.ml.entities._assets.environment.BuildContext object at 0x7fb3ba629ae0>, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOLOX_env_docker_context = Environment(\n",
    "    build=BuildContext(path=\"YOLOX-docker-contexts/python-and-pip\"),\n",
    "    name=\"YOLOX-docker-context-example\",\n",
    "    description=\"YOLOX Environment created from a Docker context.\",\n",
    ")\n",
    "ml_client.environments.create_or_update(YOLOX_env_docker_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clone YOLOX GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'YOLOX'...\n",
      "remote: Enumerating objects: 1925, done.\u001b[K\n",
      "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 1925 (delta 1), reused 7 (delta 0), pack-reused 1908 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1925/1925), 7.55 MiB | 4.93 MiB/s, done.\n",
      "Resolving deltas: 100% (1147/1147), done.\n",
      "Updating files: 100% (169/169), done.\n",
      "warning: the following paths have collided (e.g. case-sensitive paths\n",
      "on a case-insensitive filesystem) and only one from the same\n",
      "colliding group is in the working tree:\n",
      "\n",
      "  'demo/ncnn/android/app/src/main/java/com/megvii/yoloXncnn/YOLOXncnn.java'\n",
      "  'demo/ncnn/android/app/src/main/java/com/megvii/yoloXncnn/yoloXncnn.java'\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Megvii-BaseDetection/YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 37\n",
      "drwxrwxrwx 2 root root     0 Oct  7 12:34 ..\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:24 .\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:24 .git\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 .github\n",
      "-rwxrwxrwx 1 root root  3237 Oct 31 08:25 .gitignore\n",
      "-rwxrwxrwx 1 root root  1118 Oct 31 08:25 .pre-commit-config.yaml\n",
      "-rwxrwxrwx 1 root root   537 Oct 31 08:25 .readthedocs.yaml\n",
      "-rwxrwxrwx 1 root root 11371 Oct 31 08:25 LICENSE\n",
      "-rwxrwxrwx 1 root root    75 Oct 31 08:25 MANIFEST.in\n",
      "-rwxrwxrwx 1 root root 13418 Oct 31 08:25 README.md\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 assets\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 datasets\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 demo\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 docs\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 exps\n",
      "-rwxrwxrwx 1 root root   498 Oct 31 08:25 hubconf.py\n",
      "-rwxrwxrwx 1 root root   277 Oct 31 08:25 requirements.txt\n",
      "-rwxrwxrwx 1 root root   618 Oct 31 08:25 setup.cfg\n",
      "-rwxrwxrwx 1 root root  2710 Oct 31 08:25 setup.py\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 tests\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 tools\n",
      "drwxrwxrwx 2 root root     0 Oct 31 08:25 yolox\n"
     ]
    }
   ],
   "source": [
    "!ls -rtla YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f YOLOX/.gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Get YOLOX-s weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-31 08:25:23--  https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/386811486/42c4cb47-f94e-475b-a3a2-57f31f26fa5d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241031%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241031T082523Z&X-Amz-Expires=300&X-Amz-Signature=d784f2835954cdda29b7463d80f3277a1815939b05bd0cc6b38146e80c0fa6f0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolox_s.pth&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-10-31 08:25:23--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/386811486/42c4cb47-f94e-475b-a3a2-57f31f26fa5d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241031%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241031T082523Z&X-Amz-Expires=300&X-Amz-Signature=d784f2835954cdda29b7463d80f3277a1815939b05bd0cc6b38146e80c0fa6f0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolox_s.pth&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72089125 (69M) [application/octet-stream]\n",
      "Saving to: ‘yolox_s.pth’\n",
      "\n",
      "yolox_s.pth         100%[===================>]  68.75M   282MB/s    in 0.2s    \n",
      "\n",
      "2024-10-31 08:25:24 (282 MB/s) - ‘yolox_s.pth’ saved [72089125/72089125]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd YOLOX;wget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Customize your training Script\n",
    "\n",
    "We added `--output_dir` argument to control output directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./YOLOX/tools/my_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./YOLOX/tools/my_demo.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-\n",
    "# Copyright (c) Megvii, Inc. and its affiliates.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from loguru import logger\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "from yolox.data.data_augment import ValTransform\n",
    "from yolox.data.datasets import COCO_CLASSES\n",
    "from yolox.exp import get_exp\n",
    "from yolox.utils import fuse_model, get_model_info, postprocess, vis\n",
    "\n",
    "IMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]\n",
    "\n",
    "\n",
    "def make_parser():\n",
    "    parser = argparse.ArgumentParser(\"YOLOX Demo!\")\n",
    "    parser.add_argument(\n",
    "        \"demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\"\n",
    "    )\n",
    "    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n",
    "    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--path\", default=\"./assets/dog.jpg\", help=\"path to images or video\"\n",
    "    )\n",
    "    parser.add_argument(\"--camid\", type=int, default=0, help=\"webcam demo camera id\")\n",
    "    parser.add_argument(\n",
    "        \"--save_result\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to save the inference result of image/video\",\n",
    "    )\n",
    "\n",
    "    # exp file\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--exp_file\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"please input your experiment description file\",\n",
    "    )\n",
    "    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        default=\"cpu\",\n",
    "        type=str,\n",
    "        help=\"device to run our model, can either be cpu or gpu\",\n",
    "    )\n",
    "    parser.add_argument(\"--conf\", default=0.3, type=float, help=\"test conf\")\n",
    "    parser.add_argument(\"--nms\", default=0.3, type=float, help=\"test nms threshold\")\n",
    "    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        dest=\"fp16\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Adopting mix precision evaluating.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--legacy\",\n",
    "        dest=\"legacy\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"To be compatible with older versions\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fuse\",\n",
    "        dest=\"fuse\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Fuse conv and bn for testing.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--trt\",\n",
    "        dest=\"trt\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Using TensorRT model for testing.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--output_dir\", type=str, help=\"output directory\")\n",
    "    \n",
    "    return parser\n",
    "\n",
    "\n",
    "def get_image_list(path):\n",
    "    image_names = []\n",
    "    for maindir, subdir, file_name_list in os.walk(path):\n",
    "        for filename in file_name_list:\n",
    "            apath = os.path.join(maindir, filename)\n",
    "            ext = os.path.splitext(apath)[1]\n",
    "            if ext in IMAGE_EXT:\n",
    "                image_names.append(apath)\n",
    "    return image_names\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        exp,\n",
    "        cls_names=COCO_CLASSES,\n",
    "        trt_file=None,\n",
    "        decoder=None,\n",
    "        device=\"cpu\",\n",
    "        fp16=False,\n",
    "        legacy=False,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.cls_names = cls_names\n",
    "        self.decoder = decoder\n",
    "        self.num_classes = exp.num_classes\n",
    "        self.confthre = exp.test_conf\n",
    "        self.nmsthre = exp.nmsthre\n",
    "        self.test_size = exp.test_size\n",
    "        self.device = device\n",
    "        self.fp16 = fp16\n",
    "        self.preproc = ValTransform(legacy=legacy)\n",
    "        if trt_file is not None:\n",
    "            from torch2trt import TRTModule\n",
    "\n",
    "            model_trt = TRTModule()\n",
    "            model_trt.load_state_dict(torch.load(trt_file))\n",
    "\n",
    "            x = torch.ones(1, 3, exp.test_size[0], exp.test_size[1]).cuda()\n",
    "            self.model(x)\n",
    "            self.model = model_trt\n",
    "\n",
    "    def inference(self, img):\n",
    "        img_info = {\"id\": 0}\n",
    "        if isinstance(img, str):\n",
    "            img_info[\"file_name\"] = os.path.basename(img)\n",
    "            img = cv2.imread(img)\n",
    "        else:\n",
    "            img_info[\"file_name\"] = None\n",
    "\n",
    "        height, width = img.shape[:2]\n",
    "        img_info[\"height\"] = height\n",
    "        img_info[\"width\"] = width\n",
    "        img_info[\"raw_img\"] = img\n",
    "\n",
    "        ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])\n",
    "        img_info[\"ratio\"] = ratio\n",
    "\n",
    "        img, _ = self.preproc(img, None, self.test_size)\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        img = img.float()\n",
    "        if self.device == \"gpu\":\n",
    "            img = img.cuda()\n",
    "            if self.fp16:\n",
    "                img = img.half()  # to FP16\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t0 = time.time()\n",
    "            outputs = self.model(img)\n",
    "            if self.decoder is not None:\n",
    "                outputs = self.decoder(outputs, dtype=outputs.type())\n",
    "            outputs = postprocess(\n",
    "                outputs, self.num_classes, self.confthre,\n",
    "                self.nmsthre, class_agnostic=True\n",
    "            )\n",
    "            logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n",
    "        return outputs, img_info\n",
    "\n",
    "    def visual(self, output, img_info, cls_conf=0.35):\n",
    "        ratio = img_info[\"ratio\"]\n",
    "        img = img_info[\"raw_img\"]\n",
    "        if output is None:\n",
    "            return img\n",
    "        output = output.cpu()\n",
    "\n",
    "        bboxes = output[:, 0:4]\n",
    "\n",
    "        # preprocessing: resize\n",
    "        bboxes /= ratio\n",
    "\n",
    "        cls = output[:, 6]\n",
    "        scores = output[:, 4] * output[:, 5]\n",
    "\n",
    "        vis_res = vis(img, bboxes, scores, cls, cls_conf, self.cls_names)\n",
    "        return vis_res\n",
    "\n",
    "\n",
    "def image_demo(predictor, vis_folder, path, current_time, save_result):\n",
    "    if os.path.isdir(path):\n",
    "        files = get_image_list(path)\n",
    "    else:\n",
    "        files = [path]\n",
    "    files.sort()\n",
    "    for image_name in files:\n",
    "        outputs, img_info = predictor.inference(image_name)\n",
    "        result_image = predictor.visual(outputs[0], img_info, predictor.confthre)\n",
    "        if save_result:\n",
    "            save_folder = os.path.join(\n",
    "                vis_folder, time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
    "            )\n",
    "            os.makedirs(save_folder, exist_ok=True)\n",
    "            save_file_name = os.path.join(save_folder, os.path.basename(image_name))\n",
    "            logger.info(\"Saving detection result in {}\".format(save_file_name))\n",
    "            cv2.imwrite(save_file_name, result_image)\n",
    "        ch = cv2.waitKey(0)\n",
    "        if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
    "            break\n",
    "\n",
    "\n",
    "def imageflow_demo(predictor, vis_folder, current_time, args):\n",
    "    cap = cv2.VideoCapture(args.path if args.demo == \"video\" else args.camid)\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # float\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if args.save_result:\n",
    "        save_folder = os.path.join(\n",
    "            vis_folder, time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
    "        )\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        if args.demo == \"video\":\n",
    "            save_path = os.path.join(save_folder, os.path.basename(args.path))\n",
    "        else:\n",
    "            save_path = os.path.join(save_folder, \"camera.mp4\")\n",
    "        logger.info(f\"video save_path is {save_path}\")\n",
    "        vid_writer = cv2.VideoWriter(\n",
    "            save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (int(width), int(height))\n",
    "        )\n",
    "    while True:\n",
    "        ret_val, frame = cap.read()\n",
    "        if ret_val:\n",
    "            outputs, img_info = predictor.inference(frame)\n",
    "            result_frame = predictor.visual(outputs[0], img_info, predictor.confthre)\n",
    "            if args.save_result:\n",
    "                vid_writer.write(result_frame)\n",
    "            else:\n",
    "                cv2.namedWindow(\"yolox\", cv2.WINDOW_NORMAL)\n",
    "                cv2.imshow(\"yolox\", result_frame)\n",
    "            ch = cv2.waitKey(1)\n",
    "            if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "def main(exp, args):\n",
    "    if not args.experiment_name:\n",
    "        args.experiment_name = exp.exp_name\n",
    "\n",
    "    file_name = os.path.join(exp.output_dir, args.experiment_name)\n",
    "    os.makedirs(file_name, exist_ok=True)\n",
    "\n",
    "    vis_folder = None\n",
    "    if args.save_result:\n",
    "        if args.output_dir:\n",
    "            vis_folder = args.output_dir\n",
    "        else:\n",
    "            vis_folder = os.path.join(file_name, \"vis_res\")\n",
    "        print(f\"vis_folder: {vis_folder}\")\n",
    "        os.makedirs(vis_folder, exist_ok=True)\n",
    "\n",
    "    if args.trt:\n",
    "        args.device = \"gpu\"\n",
    "\n",
    "    logger.info(\"Args: {}\".format(args))\n",
    "\n",
    "    if args.conf is not None:\n",
    "        exp.test_conf = args.conf\n",
    "    if args.nms is not None:\n",
    "        exp.nmsthre = args.nms\n",
    "    if args.tsize is not None:\n",
    "        exp.test_size = (args.tsize, args.tsize)\n",
    "\n",
    "    model = exp.get_model()\n",
    "    logger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n",
    "\n",
    "    if args.device == \"gpu\":\n",
    "        model.cuda()\n",
    "        if args.fp16:\n",
    "            model.half()  # to FP16\n",
    "    model.eval()\n",
    "\n",
    "    if not args.trt:\n",
    "        if args.ckpt is None:\n",
    "            ckpt_file = os.path.join(file_name, \"best_ckpt.pth\")\n",
    "        else:\n",
    "            ckpt_file = args.ckpt\n",
    "        logger.info(\"loading checkpoint\")\n",
    "        ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n",
    "        # load the model state dict\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        logger.info(\"loaded checkpoint done.\")\n",
    "\n",
    "    if args.fuse:\n",
    "        logger.info(\"\\tFusing model...\")\n",
    "        model = fuse_model(model)\n",
    "\n",
    "    if args.trt:\n",
    "        assert not args.fuse, \"TensorRT model is not support model fusing!\"\n",
    "        trt_file = os.path.join(file_name, \"model_trt.pth\")\n",
    "        assert os.path.exists(\n",
    "            trt_file\n",
    "        ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n",
    "        model.head.decode_in_inference = False\n",
    "        decoder = model.head.decode_outputs\n",
    "        logger.info(\"Using TensorRT to inference\")\n",
    "    else:\n",
    "        trt_file = None\n",
    "        decoder = None\n",
    "\n",
    "    predictor = Predictor(\n",
    "        model, exp, COCO_CLASSES, trt_file, decoder,\n",
    "        args.device, args.fp16, args.legacy,\n",
    "    )\n",
    "    current_time = time.localtime()\n",
    "    if args.demo == \"image\":\n",
    "        image_demo(predictor, vis_folder, args.path, current_time, args.save_result)\n",
    "    elif args.demo == \"video\" or args.demo == \"webcam\":\n",
    "        imageflow_demo(predictor, vis_folder, current_time, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = make_parser().parse_args()\n",
    "    exp = get_exp(args.exp_file, args.name)\n",
    "\n",
    "    main(exp, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env python3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# -*- coding:utf-8 -*-\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Copyright (c) Megvii, Inc. and its affiliates.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mloguru\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m logger\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcv2\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36myolox\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata_augment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ValTransform\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36myolox\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m COCO_CLASSES\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36myolox\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexp\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_exp\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36myolox\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m fuse_model, get_model_info, postprocess, vis\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "IMAGE_EXT = [\u001b[33m\"\u001b[39;49;00m\u001b[33m.jpg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m.jpeg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m.webp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m.bmp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmake_parser\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(\u001b[33m\"\u001b[39;49;00m\u001b[33mYOLOX Demo!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mdemo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mdemo type, eg. image, video and webcam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m-expn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m--experiment-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m-n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m--name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m./assets/dog.jpg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mpath to images or video\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--camid\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwebcam demo camera id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--save_result\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to save the inference result of image/video\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# exp file\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m-f\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--exp_file\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mplease input your experiment description file\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m--ckpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mckpt for eval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--device\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice to run our model, can either be cpu or gpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--conf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m0.3\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mtest conf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--nms\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m0.3\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mtest nms threshold\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--tsize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mtest img size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        dest=\u001b[33m\"\u001b[39;49;00m\u001b[33mfp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mAdopting mix precision evaluating.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--legacy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        dest=\u001b[33m\"\u001b[39;49;00m\u001b[33mlegacy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mTo be compatible with older versions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--fuse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        dest=\u001b[33m\"\u001b[39;49;00m\u001b[33mfuse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mFuse conv and bn for testing.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--trt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        dest=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing TensorRT model for testing.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moutput directory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_image_list\u001b[39;49;00m(path):\u001b[37m\u001b[39;49;00m\n",
      "    image_names = []\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m maindir, subdir, file_name_list \u001b[35min\u001b[39;49;00m os.walk(path):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m filename \u001b[35min\u001b[39;49;00m file_name_list:\u001b[37m\u001b[39;49;00m\n",
      "            apath = os.path.join(maindir, filename)\u001b[37m\u001b[39;49;00m\n",
      "            ext = os.path.splitext(apath)[\u001b[34m1\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m ext \u001b[35min\u001b[39;49;00m IMAGE_EXT:\u001b[37m\u001b[39;49;00m\n",
      "                image_names.append(apath)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m image_names\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mPredictor\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        model,\u001b[37m\u001b[39;49;00m\n",
      "        exp,\u001b[37m\u001b[39;49;00m\n",
      "        cls_names=COCO_CLASSES,\u001b[37m\u001b[39;49;00m\n",
      "        trt_file=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        decoder=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        device=\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        fp16=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        legacy=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    ):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.model = model\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.cls_names = cls_names\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.decoder = decoder\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.num_classes = exp.num_classes\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.confthre = exp.test_conf\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.nmsthre = exp.nmsthre\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.test_size = exp.test_size\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.device = device\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.fp16 = fp16\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.preproc = ValTransform(legacy=legacy)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m trt_file \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch2trt\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TRTModule\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            model_trt = TRTModule()\u001b[37m\u001b[39;49;00m\n",
      "            model_trt.load_state_dict(torch.load(trt_file))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            x = torch.ones(\u001b[34m1\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, exp.test_size[\u001b[34m0\u001b[39;49;00m], exp.test_size[\u001b[34m1\u001b[39;49;00m]).cuda()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.model(x)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.model = model_trt\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minference\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, img):\u001b[37m\u001b[39;49;00m\n",
      "        img_info = {\u001b[33m\"\u001b[39;49;00m\u001b[33mid\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(img, \u001b[36mstr\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            img_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mfile_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = os.path.basename(img)\u001b[37m\u001b[39;49;00m\n",
      "            img = cv2.imread(img)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            img_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mfile_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        height, width = img.shape[:\u001b[34m2\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        img_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mheight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = height\u001b[37m\u001b[39;49;00m\n",
      "        img_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mwidth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = width\u001b[37m\u001b[39;49;00m\n",
      "        img_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mraw_img\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = img\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        ratio = \u001b[36mmin\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.test_size[\u001b[34m0\u001b[39;49;00m] / img.shape[\u001b[34m0\u001b[39;49;00m], \u001b[36mself\u001b[39;49;00m.test_size[\u001b[34m1\u001b[39;49;00m] / img.shape[\u001b[34m1\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "        img_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = ratio\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        img, _ = \u001b[36mself\u001b[39;49;00m.preproc(img, \u001b[34mNone\u001b[39;49;00m, \u001b[36mself\u001b[39;49;00m.test_size)\u001b[37m\u001b[39;49;00m\n",
      "        img = torch.from_numpy(img).unsqueeze(\u001b[34m0\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        img = img.float()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.device == \u001b[33m\"\u001b[39;49;00m\u001b[33mgpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            img = img.cuda()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.fp16:\u001b[37m\u001b[39;49;00m\n",
      "                img = img.half()  \u001b[37m# to FP16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m torch.no_grad():\u001b[37m\u001b[39;49;00m\n",
      "            t0 = time.time()\u001b[37m\u001b[39;49;00m\n",
      "            outputs = \u001b[36mself\u001b[39;49;00m.model(img)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.decoder \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                outputs = \u001b[36mself\u001b[39;49;00m.decoder(outputs, dtype=outputs.type())\u001b[37m\u001b[39;49;00m\n",
      "            outputs = postprocess(\u001b[37m\u001b[39;49;00m\n",
      "                outputs, \u001b[36mself\u001b[39;49;00m.num_classes, \u001b[36mself\u001b[39;49;00m.confthre,\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mself\u001b[39;49;00m.nmsthre, class_agnostic=\u001b[34mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mInfer time: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33ms\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(time.time() - t0))\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m outputs, img_info\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mvisual\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, output, img_info, cls_conf=\u001b[34m0.35\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        ratio = img_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        img = img_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mraw_img\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m output \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mreturn\u001b[39;49;00m img\u001b[37m\u001b[39;49;00m\n",
      "        output = output.cpu()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        bboxes = output[:, \u001b[34m0\u001b[39;49;00m:\u001b[34m4\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# preprocessing: resize\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        bboxes /= ratio\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mcls\u001b[39;49;00m = output[:, \u001b[34m6\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        scores = output[:, \u001b[34m4\u001b[39;49;00m] * output[:, \u001b[34m5\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        vis_res = vis(img, bboxes, scores, \u001b[36mcls\u001b[39;49;00m, cls_conf, \u001b[36mself\u001b[39;49;00m.cls_names)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m vis_res\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mimage_demo\u001b[39;49;00m(predictor, vis_folder, path, current_time, save_result):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m os.path.isdir(path):\u001b[37m\u001b[39;49;00m\n",
      "        files = get_image_list(path)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        files = [path]\u001b[37m\u001b[39;49;00m\n",
      "    files.sort()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m image_name \u001b[35min\u001b[39;49;00m files:\u001b[37m\u001b[39;49;00m\n",
      "        outputs, img_info = predictor.inference(image_name)\u001b[37m\u001b[39;49;00m\n",
      "        result_image = predictor.visual(outputs[\u001b[34m0\u001b[39;49;00m], img_info, predictor.confthre)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m save_result:\u001b[37m\u001b[39;49;00m\n",
      "            save_folder = os.path.join(\u001b[37m\u001b[39;49;00m\n",
      "                vis_folder, time.strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm_\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, current_time)\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "            os.makedirs(save_folder, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            save_file_name = os.path.join(save_folder, os.path.basename(image_name))\u001b[37m\u001b[39;49;00m\n",
      "            logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving detection result in \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(save_file_name))\u001b[37m\u001b[39;49;00m\n",
      "            cv2.imwrite(save_file_name, result_image)\u001b[37m\u001b[39;49;00m\n",
      "        ch = cv2.waitKey(\u001b[34m0\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m ch == \u001b[34m27\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m ch == \u001b[36mord\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mq\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[35mor\u001b[39;49;00m ch == \u001b[36mord\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mQ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mbreak\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mimageflow_demo\u001b[39;49;00m(predictor, vis_folder, current_time, args):\u001b[37m\u001b[39;49;00m\n",
      "    cap = cv2.VideoCapture(args.path \u001b[34mif\u001b[39;49;00m args.demo == \u001b[33m\"\u001b[39;49;00m\u001b[33mvideo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m args.camid)\u001b[37m\u001b[39;49;00m\n",
      "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  \u001b[37m# float\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  \u001b[37m# float\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    fps = cap.get(cv2.CAP_PROP_FPS)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.save_result:\u001b[37m\u001b[39;49;00m\n",
      "        save_folder = os.path.join(\u001b[37m\u001b[39;49;00m\n",
      "            vis_folder, time.strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm_\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, current_time)\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        os.makedirs(save_folder, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m args.demo == \u001b[33m\"\u001b[39;49;00m\u001b[33mvideo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            save_path = os.path.join(save_folder, os.path.basename(args.path))\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            save_path = os.path.join(save_folder, \u001b[33m\"\u001b[39;49;00m\u001b[33mcamera.mp4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mvideo save_path is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msave_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        vid_writer = cv2.VideoWriter(\u001b[37m\u001b[39;49;00m\n",
      "            save_path, cv2.VideoWriter_fourcc(*\u001b[33m\"\u001b[39;49;00m\u001b[33mmp4v\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), fps, (\u001b[36mint\u001b[39;49;00m(width), \u001b[36mint\u001b[39;49;00m(height))\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwhile\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        ret_val, frame = cap.read()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m ret_val:\u001b[37m\u001b[39;49;00m\n",
      "            outputs, img_info = predictor.inference(frame)\u001b[37m\u001b[39;49;00m\n",
      "            result_frame = predictor.visual(outputs[\u001b[34m0\u001b[39;49;00m], img_info, predictor.confthre)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m args.save_result:\u001b[37m\u001b[39;49;00m\n",
      "                vid_writer.write(result_frame)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                cv2.namedWindow(\u001b[33m\"\u001b[39;49;00m\u001b[33myolox\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, cv2.WINDOW_NORMAL)\u001b[37m\u001b[39;49;00m\n",
      "                cv2.imshow(\u001b[33m\"\u001b[39;49;00m\u001b[33myolox\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, result_frame)\u001b[37m\u001b[39;49;00m\n",
      "            ch = cv2.waitKey(\u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m ch == \u001b[34m27\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m ch == \u001b[36mord\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mq\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[35mor\u001b[39;49;00m ch == \u001b[36mord\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mQ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[34mbreak\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mbreak\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m(exp, args):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.experiment_name:\u001b[37m\u001b[39;49;00m\n",
      "        args.experiment_name = exp.exp_name\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    file_name = os.path.join(exp.output_dir, args.experiment_name)\u001b[37m\u001b[39;49;00m\n",
      "    os.makedirs(file_name, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    vis_folder = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.save_result:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m args.output_dir:\u001b[37m\u001b[39;49;00m\n",
      "            vis_folder = args.output_dir\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            vis_folder = os.path.join(file_name, \u001b[33m\"\u001b[39;49;00m\u001b[33mvis_res\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mvis_folder: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvis_folder\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        os.makedirs(vis_folder, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.trt:\u001b[37m\u001b[39;49;00m\n",
      "        args.device = \u001b[33m\"\u001b[39;49;00m\u001b[33mgpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mArgs: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.conf \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        exp.test_conf = args.conf\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.nms \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        exp.nmsthre = args.nms\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.tsize \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        exp.test_size = (args.tsize, args.tsize)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model = exp.get_model()\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel Summary: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(get_model_info(model, exp.test_size)))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.device == \u001b[33m\"\u001b[39;49;00m\u001b[33mgpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        model.cuda()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m args.fp16:\u001b[37m\u001b[39;49;00m\n",
      "            model.half()  \u001b[37m# to FP16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model.eval()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.trt:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m args.ckpt \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            ckpt_file = os.path.join(file_name, \u001b[33m\"\u001b[39;49;00m\u001b[33mbest_ckpt.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            ckpt_file = args.ckpt\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mloading checkpoint\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        ckpt = torch.load(ckpt_file, map_location=\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# load the model state dict\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model.load_state_dict(ckpt[\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mloaded checkpoint done.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.fuse:\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mFusing model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        model = fuse_model(model)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.trt:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.fuse, \u001b[33m\"\u001b[39;49;00m\u001b[33mTensorRT model is not support model fusing!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        trt_file = os.path.join(file_name, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_trt.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34massert\u001b[39;49;00m os.path.exists(\u001b[37m\u001b[39;49;00m\n",
      "            trt_file\u001b[37m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mTensorRT model is not found!\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m Run python3 tools/trt.py first!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model.head.decode_in_inference = \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        decoder = model.head.decode_outputs\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing TensorRT to inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trt_file = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        decoder = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    predictor = Predictor(\u001b[37m\u001b[39;49;00m\n",
      "        model, exp, COCO_CLASSES, trt_file, decoder,\u001b[37m\u001b[39;49;00m\n",
      "        args.device, args.fp16, args.legacy,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    current_time = time.localtime()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.demo == \u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        image_demo(predictor, vis_folder, args.path, current_time, args.save_result)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melif\u001b[39;49;00m args.demo == \u001b[33m\"\u001b[39;49;00m\u001b[33mvideo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m args.demo == \u001b[33m\"\u001b[39;49;00m\u001b[33mwebcam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        imageflow_demo(predictor, vis_folder, current_time, args)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    args = make_parser().parse_args()\u001b[37m\u001b[39;49;00m\n",
      "    exp = get_exp(args.exp_file, args.name)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    main(exp, args)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./YOLOX/tools/my_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create a Compute Resource to run our job\n",
    "\n",
    "AzureML needs a compute resource for running a job. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
    "\n",
    "In this example, we provision a Linux [compute cluster](https://docs.microsoft.com/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python). See the [full list on VM sizes and prices](https://azure.microsoft.com/pricing/details/machine-learning/) .\n",
    "\n",
    "For this example we need a gpu cluster, let's pick a Standard_NC4as_T4_v3 model and create an Azure ML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already have a cluster named gpu-cluster-nc4as, we'll reuse it as is.\n",
      "AMLCompute with name gpu-cluster-nc4as is created, the compute size is Standard_NC4as_T4_v3\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "gpu_compute_target = \"gpu-cluster-nc4as\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    gpu_cluster = ml_client.compute.get(gpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {gpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new gpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure ML compute object with the intended parameters\n",
    "    gpu_cluster = AmlCompute(\n",
    "        # Name assigned to the compute cluster\n",
    "        name=\"gpu-cluster-nc4as\",\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"Standard_NC4as_T4_v3\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=2,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=600,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Configure the Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        num_epochs=30, \n",
    "        learning_rate=0.001, \n",
    "        momentum=0.9, \n",
    "        output_dir=\"./outputs\"\n",
    "    ),\n",
    "    compute=gpu_compute_target,\n",
    "    environment=\"YOLOX-docker-context-example:3\",\n",
    "    code=\"./YOLOX/\",  # location of source code\n",
    "    command=\"python tools/my_demo.py image -n yolox-s -c yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu] --output_dir ${{inputs.output_dir}}\",\n",
    "    experiment_name=\"YOLOX-experiment\",\n",
    "    display_name=\"YOLOX-run\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Submit the job\n",
    "\n",
    "It's now time to submit the job to run in AzureML. This time you'll use create_or_update on ml_client.jobs.\n",
    "\n",
    "Once completed, the job will register a model in your workspace as a result of training. You can view the job in AzureML studio by clicking on the link in the output of the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>YOLOX-experiment</td><td>stoic_screw_q0j9pbjtkx</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/stoic_screw_q0j9pbjtkx?wsid=/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourcegroups/rg-dp100-l/workspaces/mlw-dp100-l&amp;tid=16b3c013-d300-468d-ac64-7eda0820b6d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "Command({'parameters': {}, 'init': False, 'name': 'stoic_screw_q0j9pbjtkx', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'_azureml.ComputeTargetType': 'amlctrain', '_azureml.ClusterName': 'gpu-cluster-nc4as', 'ContentSnapshotId': '96ef102c-8ec8-4de2-99b4-8ceb88e867eb'}, 'print_as_yaml': False, 'id': '/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourceGroups/rg-dp100-l/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-l/jobs/stoic_screw_q0j9pbjtkx', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/my-compute-instance-a1/code/azureml-examples/sdk/python/assets/environment', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fb3a82ce920>, 'serialize': <msrest.serialization.Serializer object at 0x7fb3a8332080>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <TraceLogger attr_dict (WARNING)>, 'display_name': 'YOLOX-run', 'experiment_name': 'YOLOX-experiment', 'compute': 'gpu-cluster-nc4as', 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourceGroups/rg-dp100-l/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-l?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/stoic_screw_q0j9pbjtkx?wsid=/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourcegroups/rg-dp100-l/workspaces/mlw-dp100-l&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {'num_epochs': '30', 'learning_rate': '0.001', 'momentum': '0.9', 'output_dir': './outputs'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.stoic_screw_q0j9pbjtkx', 'mode': 'rw_mount'}}, 'inputs': {'num_epochs': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fb3a8331f30>, 'learning_rate': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fb3a8331de0>, 'momentum': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fb3a83337f0>, 'output_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fb3a8333a00>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7fb3a8333be0>}, 'component': CommandComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'stoic_screw_q0j9pbjtkx', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/my-compute-instance-a1/code/azureml-examples/sdk/python/assets/environment', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fb3a82ce920>, 'serialize': <msrest.serialization.Serializer object at 0x7fb3a8332650>, 'command': 'python tools/my_demo.py image -n yolox-s -c yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu] --output_dir ${{inputs.output_dir}}', 'code': '/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourceGroups/rg-dp100-l/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-l/codes/824acefd-9472-49ff-a254-9ce2d4bd964f/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourceGroups/rg-dp100-l/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-l/environments/YOLOX-docker-context-example/versions/3', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'schema': None, 'type': 'command', 'display_name': 'YOLOX-run', 'is_deterministic': True, 'inputs': {'num_epochs': {'type': 'string', 'default': '30'}, 'learning_rate': {'type': 'string', 'default': '0.001'}, 'momentum': {'type': 'string', 'default': '0.9'}, 'output_dir': {'type': 'string', 'default': './outputs'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.stoic_screw_q0j9pbjtkx', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': []}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourceGroups/rg-dp100-l/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-l?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/stoic_screw_q0j9pbjtkx?wsid=/subscriptions/2a2f4384-a3a7-40e2-b710-2515fa4e46ad/resourcegroups/rg-dp100-l/workspaces/mlw-dp100-l&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fb3a82ce920>}, 'instance_id': '95a01b12-6be9-4195-8af0-b0d9f0f68720', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'YOLOX-docker-context-example:3', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.jobs.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens during job execution\n",
    "\n",
    "As the job is executed, it goes through the following stages:\n",
    "\n",
    "- Preparing: A docker image is created according to the environment defined. The image is uploaded to the workspace's container registry and cached for later runs. Logs are also streamed to the job history and can be viewed to monitor progress. If a curated environment is used, the cached image backing that curated environment will be used.\n",
    "- Scaling: The cluster attempts to scale up if the cluster requires more nodes to execute the run than are currently available.\n",
    "- Running: All scripts in the src folder are uploaded to the compute target, data stores are mounted or copied, and the script is executed. Outputs from stdout and the ./logs folder are streamed to the job history and can be used to monitor the job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "description": {
   "description": "Create custom environments from docker and/or conda YAML"
  },
  "interpreter": {
   "hash": "66962d4c952b5ba37638a017d6cc83bab37d76f69b13c17d86b9f71233a0aa71"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

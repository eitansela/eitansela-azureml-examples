{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Azure Machine Learning YOLOX Custom Environment\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace - [Configure workspace](../../jobs/configuration.ipynb) \n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../README.md) - check the getting started section\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Create a custom environment from python SDK using\n",
    "  - A docker context\n",
    "\n",
    "**Motivations** - Azure Machine Learning environments are an encapsulation of the environment where your machine learning training happens. By default your workspace has several curated environments already available. This notebook explains how to create a custom environment to run your specific task if you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install azure-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msdoc": "how-to-manage-environments-v2.md",
    "name": "libraries",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Environment\n",
    "Azure Machine Learning [environments](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments) are an encapsulation of the environment where your machine learning training happens. They specify the Python packages, environment variables, and software settings around your training and scoring scripts. They also specify run times (Python, Spark, or Docker). The environments are managed and versioned entities within your Machine Learning workspace that enable reproducible, auditable, and portable machine learning workflows across a variety of computes.\n",
    "\n",
    "The workspace contains several curated environments by default to use as-is. However, you can create your own custom environment to meet your specific needs.\n",
    "\n",
    "The `Environment` class will be used to create a custom environment. It accepts the following key parameters:\n",
    "- `name` - Name of the environment.\t\t\n",
    "- `version`\t- Version of the environment. If omitted, Azure ML will autogenerate a version.\t\t\n",
    "- `image` - The Docker image to use for the environment. Either `image` or `build` is required to create environment.\n",
    "- `conda_file` - The standard conda YAML [configuration file](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-file-manually) of the dependencies for a conda environment. It can be used with a `image`. If specified, Azure ML will build the conda environment on top of the Docker image provided.\n",
    "- `BuildContext`- The Docker build context configuration to use for the environment. Either `image` or `build` is required to create environment.\n",
    "  - `path`- Local path to the directory to use as the build context.\t\t\n",
    "  - `dockerfile_path` - Relative path to the Dockerfile within the build context.\n",
    "- `description`\t- Description of the environment.\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create a custom environment from Docker build context configuration\n",
    "In this sample we will use a local docker build configuration to create an environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p YOLOX-docker-contexts/python-and-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./YOLOX-docker-contexts/python-and-pip/Dockerfile\n",
    "\n",
    "FROM mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1:18\n",
    "\n",
    "RUN git clone https://github.com/Megvii-BaseDetection/YOLOX\n",
    "WORKDIR \"YOLOX\"\n",
    "\n",
    "# python installs\n",
    "RUN pip3 install -r requirements.txt \n",
    "RUN pip3 install -v -e .\n",
    "\n",
    "WORKDIR \"/\"\n",
    "\n",
    "# set command\n",
    "CMD [\"bash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msdoc": "how-to-manage-environments-v2.md",
    "name": "create_from_docker_context",
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOLOX_env_docker_context = Environment(\n",
    "    build=BuildContext(path=\"YOLOX-docker-contexts/python-and-pip\"),\n",
    "    name=\"YOLOX-docker-context-example\",\n",
    "    description=\"YOLOX Environment created from a Docker context.\",\n",
    ")\n",
    "ml_client.environments.create_or_update(YOLOX_env_docker_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clone YOLOX GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Megvii-BaseDetection/YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -rtla YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f YOLOX/.gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Get YOLOX-s weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd YOLOX;wget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Customize your training Script\n",
    "\n",
    "We added `--output_dir` argument to control output directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./YOLOX/tools/my_demo.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-\n",
    "# Copyright (c) Megvii, Inc. and its affiliates.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from loguru import logger\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "from yolox.data.data_augment import ValTransform\n",
    "from yolox.data.datasets import COCO_CLASSES\n",
    "from yolox.exp import get_exp\n",
    "from yolox.utils import fuse_model, get_model_info, postprocess, vis\n",
    "\n",
    "IMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]\n",
    "\n",
    "\n",
    "def make_parser():\n",
    "    parser = argparse.ArgumentParser(\"YOLOX Demo!\")\n",
    "    parser.add_argument(\n",
    "        \"demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\"\n",
    "    )\n",
    "    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n",
    "    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--path\", default=\"./assets/dog.jpg\", help=\"path to images or video\"\n",
    "    )\n",
    "    parser.add_argument(\"--camid\", type=int, default=0, help=\"webcam demo camera id\")\n",
    "    parser.add_argument(\n",
    "        \"--save_result\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to save the inference result of image/video\",\n",
    "    )\n",
    "\n",
    "    # exp file\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--exp_file\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"please input your experiment description file\",\n",
    "    )\n",
    "    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        default=\"cpu\",\n",
    "        type=str,\n",
    "        help=\"device to run our model, can either be cpu or gpu\",\n",
    "    )\n",
    "    parser.add_argument(\"--conf\", default=0.3, type=float, help=\"test conf\")\n",
    "    parser.add_argument(\"--nms\", default=0.3, type=float, help=\"test nms threshold\")\n",
    "    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        dest=\"fp16\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Adopting mix precision evaluating.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--legacy\",\n",
    "        dest=\"legacy\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"To be compatible with older versions\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fuse\",\n",
    "        dest=\"fuse\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Fuse conv and bn for testing.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--trt\",\n",
    "        dest=\"trt\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Using TensorRT model for testing.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--output_dir\", type=str, help=\"output directory\")\n",
    "    \n",
    "    return parser\n",
    "\n",
    "\n",
    "def get_image_list(path):\n",
    "    image_names = []\n",
    "    for maindir, subdir, file_name_list in os.walk(path):\n",
    "        for filename in file_name_list:\n",
    "            apath = os.path.join(maindir, filename)\n",
    "            ext = os.path.splitext(apath)[1]\n",
    "            if ext in IMAGE_EXT:\n",
    "                image_names.append(apath)\n",
    "    return image_names\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        exp,\n",
    "        cls_names=COCO_CLASSES,\n",
    "        trt_file=None,\n",
    "        decoder=None,\n",
    "        device=\"cpu\",\n",
    "        fp16=False,\n",
    "        legacy=False,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.cls_names = cls_names\n",
    "        self.decoder = decoder\n",
    "        self.num_classes = exp.num_classes\n",
    "        self.confthre = exp.test_conf\n",
    "        self.nmsthre = exp.nmsthre\n",
    "        self.test_size = exp.test_size\n",
    "        self.device = device\n",
    "        self.fp16 = fp16\n",
    "        self.preproc = ValTransform(legacy=legacy)\n",
    "        if trt_file is not None:\n",
    "            from torch2trt import TRTModule\n",
    "\n",
    "            model_trt = TRTModule()\n",
    "            model_trt.load_state_dict(torch.load(trt_file))\n",
    "\n",
    "            x = torch.ones(1, 3, exp.test_size[0], exp.test_size[1]).cuda()\n",
    "            self.model(x)\n",
    "            self.model = model_trt\n",
    "\n",
    "    def inference(self, img):\n",
    "        img_info = {\"id\": 0}\n",
    "        if isinstance(img, str):\n",
    "            img_info[\"file_name\"] = os.path.basename(img)\n",
    "            img = cv2.imread(img)\n",
    "        else:\n",
    "            img_info[\"file_name\"] = None\n",
    "\n",
    "        height, width = img.shape[:2]\n",
    "        img_info[\"height\"] = height\n",
    "        img_info[\"width\"] = width\n",
    "        img_info[\"raw_img\"] = img\n",
    "\n",
    "        ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])\n",
    "        img_info[\"ratio\"] = ratio\n",
    "\n",
    "        img, _ = self.preproc(img, None, self.test_size)\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        img = img.float()\n",
    "        if self.device == \"gpu\":\n",
    "            img = img.cuda()\n",
    "            if self.fp16:\n",
    "                img = img.half()  # to FP16\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t0 = time.time()\n",
    "            outputs = self.model(img)\n",
    "            if self.decoder is not None:\n",
    "                outputs = self.decoder(outputs, dtype=outputs.type())\n",
    "            outputs = postprocess(\n",
    "                outputs, self.num_classes, self.confthre,\n",
    "                self.nmsthre, class_agnostic=True\n",
    "            )\n",
    "            logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n",
    "        return outputs, img_info\n",
    "\n",
    "    def visual(self, output, img_info, cls_conf=0.35):\n",
    "        ratio = img_info[\"ratio\"]\n",
    "        img = img_info[\"raw_img\"]\n",
    "        if output is None:\n",
    "            return img\n",
    "        output = output.cpu()\n",
    "\n",
    "        bboxes = output[:, 0:4]\n",
    "\n",
    "        # preprocessing: resize\n",
    "        bboxes /= ratio\n",
    "\n",
    "        cls = output[:, 6]\n",
    "        scores = output[:, 4] * output[:, 5]\n",
    "\n",
    "        vis_res = vis(img, bboxes, scores, cls, cls_conf, self.cls_names)\n",
    "        return vis_res\n",
    "\n",
    "\n",
    "def image_demo(predictor, vis_folder, path, current_time, save_result):\n",
    "    if os.path.isdir(path):\n",
    "        files = get_image_list(path)\n",
    "    else:\n",
    "        files = [path]\n",
    "    files.sort()\n",
    "    for image_name in files:\n",
    "        outputs, img_info = predictor.inference(image_name)\n",
    "        result_image = predictor.visual(outputs[0], img_info, predictor.confthre)\n",
    "        if save_result:\n",
    "            save_folder = os.path.join(\n",
    "                vis_folder, time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
    "            )\n",
    "            os.makedirs(save_folder, exist_ok=True)\n",
    "            save_file_name = os.path.join(save_folder, os.path.basename(image_name))\n",
    "            logger.info(\"Saving detection result in {}\".format(save_file_name))\n",
    "            cv2.imwrite(save_file_name, result_image)\n",
    "        ch = cv2.waitKey(0)\n",
    "        if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
    "            break\n",
    "\n",
    "\n",
    "def imageflow_demo(predictor, vis_folder, current_time, args):\n",
    "    cap = cv2.VideoCapture(args.path if args.demo == \"video\" else args.camid)\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # float\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if args.save_result:\n",
    "        save_folder = os.path.join(\n",
    "            vis_folder, time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
    "        )\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        if args.demo == \"video\":\n",
    "            save_path = os.path.join(save_folder, os.path.basename(args.path))\n",
    "        else:\n",
    "            save_path = os.path.join(save_folder, \"camera.mp4\")\n",
    "        logger.info(f\"video save_path is {save_path}\")\n",
    "        vid_writer = cv2.VideoWriter(\n",
    "            save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (int(width), int(height))\n",
    "        )\n",
    "    while True:\n",
    "        ret_val, frame = cap.read()\n",
    "        if ret_val:\n",
    "            outputs, img_info = predictor.inference(frame)\n",
    "            result_frame = predictor.visual(outputs[0], img_info, predictor.confthre)\n",
    "            if args.save_result:\n",
    "                vid_writer.write(result_frame)\n",
    "            else:\n",
    "                cv2.namedWindow(\"yolox\", cv2.WINDOW_NORMAL)\n",
    "                cv2.imshow(\"yolox\", result_frame)\n",
    "            ch = cv2.waitKey(1)\n",
    "            if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "def main(exp, args):\n",
    "    if not args.experiment_name:\n",
    "        args.experiment_name = exp.exp_name\n",
    "\n",
    "    file_name = os.path.join(exp.output_dir, args.experiment_name)\n",
    "    os.makedirs(file_name, exist_ok=True)\n",
    "\n",
    "    vis_folder = None\n",
    "    if args.save_result:\n",
    "        if args.output_dir:\n",
    "            vis_folder = args.output_dir\n",
    "        else:\n",
    "            vis_folder = os.path.join(file_name, \"vis_res\")\n",
    "        print(f\"vis_folder: {vis_folder}\")\n",
    "        os.makedirs(vis_folder, exist_ok=True)\n",
    "\n",
    "    if args.trt:\n",
    "        args.device = \"gpu\"\n",
    "\n",
    "    logger.info(\"Args: {}\".format(args))\n",
    "\n",
    "    if args.conf is not None:\n",
    "        exp.test_conf = args.conf\n",
    "    if args.nms is not None:\n",
    "        exp.nmsthre = args.nms\n",
    "    if args.tsize is not None:\n",
    "        exp.test_size = (args.tsize, args.tsize)\n",
    "\n",
    "    model = exp.get_model()\n",
    "    logger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n",
    "\n",
    "    if args.device == \"gpu\":\n",
    "        model.cuda()\n",
    "        if args.fp16:\n",
    "            model.half()  # to FP16\n",
    "    model.eval()\n",
    "\n",
    "    if not args.trt:\n",
    "        if args.ckpt is None:\n",
    "            ckpt_file = os.path.join(file_name, \"best_ckpt.pth\")\n",
    "        else:\n",
    "            ckpt_file = args.ckpt\n",
    "        logger.info(\"loading checkpoint\")\n",
    "        ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n",
    "        # load the model state dict\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        logger.info(\"loaded checkpoint done.\")\n",
    "\n",
    "    if args.fuse:\n",
    "        logger.info(\"\\tFusing model...\")\n",
    "        model = fuse_model(model)\n",
    "\n",
    "    if args.trt:\n",
    "        assert not args.fuse, \"TensorRT model is not support model fusing!\"\n",
    "        trt_file = os.path.join(file_name, \"model_trt.pth\")\n",
    "        assert os.path.exists(\n",
    "            trt_file\n",
    "        ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n",
    "        model.head.decode_in_inference = False\n",
    "        decoder = model.head.decode_outputs\n",
    "        logger.info(\"Using TensorRT to inference\")\n",
    "    else:\n",
    "        trt_file = None\n",
    "        decoder = None\n",
    "\n",
    "    predictor = Predictor(\n",
    "        model, exp, COCO_CLASSES, trt_file, decoder,\n",
    "        args.device, args.fp16, args.legacy,\n",
    "    )\n",
    "    current_time = time.localtime()\n",
    "    if args.demo == \"image\":\n",
    "        image_demo(predictor, vis_folder, args.path, current_time, args.save_result)\n",
    "    elif args.demo == \"video\" or args.demo == \"webcam\":\n",
    "        imageflow_demo(predictor, vis_folder, current_time, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = make_parser().parse_args()\n",
    "    exp = get_exp(args.exp_file, args.name)\n",
    "\n",
    "    main(exp, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./YOLOX/tools/my_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create a Compute Resource to run our job\n",
    "\n",
    "AzureML needs a compute resource for running a job. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
    "\n",
    "In this example, we provision a Linux [compute cluster](https://docs.microsoft.com/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python). See the [full list on VM sizes and prices](https://azure.microsoft.com/pricing/details/machine-learning/) .\n",
    "\n",
    "For this example we need a gpu cluster, let's pick a Standard_NC4as_T4_v3 model and create an Azure ML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "gpu_compute_target = \"gpu-cluster-nc4as\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    gpu_cluster = ml_client.compute.get(gpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {gpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new gpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure ML compute object with the intended parameters\n",
    "    gpu_cluster = AmlCompute(\n",
    "        # Name assigned to the compute cluster\n",
    "        name=\"gpu-cluster-nc4as\",\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"Standard_NC4as_T4_v3\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=2,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=600,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Configure the Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        num_epochs=30, \n",
    "        learning_rate=0.001, \n",
    "        momentum=0.9, \n",
    "        output_dir=\"./outputs\"\n",
    "    ),\n",
    "    compute=gpu_compute_target,\n",
    "    environment=\"YOLOX-docker-context-example:1\",\n",
    "    code=\"./YOLOX/\",  # location of source code\n",
    "    command=\"python tools/my_demo.py image -n yolox-s -c yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu] --output_dir ${{inputs.output_dir}}\",\n",
    "    experiment_name=\"YOLOX-experiment\",\n",
    "    display_name=\"YOLOX-run\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Submit the job\n",
    "\n",
    "It's now time to submit the job to run in AzureML. This time you'll use create_or_update on ml_client.jobs.\n",
    "\n",
    "Once completed, the job will register a model in your workspace as a result of training. You can view the job in AzureML studio by clicking on the link in the output of the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ml_client.jobs.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens during job execution\n",
    "\n",
    "As the job is executed, it goes through the following stages:\n",
    "\n",
    "- Preparing: A docker image is created according to the environment defined. The image is uploaded to the workspace's container registry and cached for later runs. Logs are also streamed to the job history and can be viewed to monitor progress. If a curated environment is used, the cached image backing that curated environment will be used.\n",
    "- Scaling: The cluster attempts to scale up if the cluster requires more nodes to execute the run than are currently available.\n",
    "- Running: All scripts in the src folder are uploaded to the compute target, data stores are mounted or copied, and the script is executed. Outputs from stdout and the ./logs folder are streamed to the job history and can be used to monitor the job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "description": {
   "description": "Create custom environments from docker and/or conda YAML"
  },
  "interpreter": {
   "hash": "66962d4c952b5ba37638a017d6cc83bab37d76f69b13c17d86b9f71233a0aa71"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

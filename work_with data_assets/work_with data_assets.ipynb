{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Work with Data Assets\n",
    "\n",
    "Data is the foundation on which machine learning models are built. Managing data centrally in the cloud, and making it accessible to teams of data scientists who are running experiments and training models on multiple workstations and compute targets is an important part of any professional data science solution.\n",
    "\n",
    "In this notebook, you'll explore two Azure Machine Learning objects for working with data: *datastores*, and *data assets*.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "You'll need the latest version of the **azure-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
    "\n",
    "> **Note**:\n",
    "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1666789326586
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pip show azure-ai-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Connect to your workspace\n",
    "\n",
    "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1666789343369
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## List the datastores\n",
    "\n",
    "When you create the Azure Machine Learning workspace, an Azure Storage Account is created too. The Storage Account includes Blob and file storage and are automatically connected with your workspace as **datastores**. You can list all datastores connected to your workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1666790805418
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stores = ml_client.datastores.list()\n",
    "for ds_name in stores:\n",
    "    print(ds_name.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `workspaceblobstore` which connects to the **azureml-blobstore-...** container you explored earlier. The `workspacefilestore` connects to the **code-...** file share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create a datastore\n",
    "\n",
    "Whenever you want to connect another Azure storage service with the Azure Machine Learning workspace, you can create a datastore. Note that creating a datastore, creates the connection between your workspace and the storage, it doesn't create the storage service itself. \n",
    "\n",
    "To create a datastore and connect to a (already existing) storage, you'll need to specify:\n",
    "\n",
    "- The class to indicate with what type of storage service you want to connect. The example below connects to a Blob storage (`AzureBlobDatastore`).\n",
    "- `name`: The display name of the datastore in the Azure Machine Learning workspace.\n",
    "- `description`: Optional description to provide more information about the datastore.\n",
    "- `account_name`: The name of the Azure Storage Account.\n",
    "- `container_name`: The name of the container to store blobs in the Azure Storage Account.\n",
    "- `credentials`: Provide the method of authentication and the credentials to authenticate. The example below uses an account key.\n",
    "\n",
    "**Important**: \n",
    "- Replace the **YOUR-STORAGE-ACCOUNT-NAME** with the name of the Storage Account that was automatically created for you. \n",
    "- Replace the **XXXX-XXXX** for `account_key` with the account key of your Azure Storage Account. \n",
    "\n",
    "Remember you can retrieve the account key by navigating to the [Azure portal](https://portal.azure.com), go to your Storage Account, from the **Access keys** tab, copy the **Key** value for key1 or key2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1666790818340
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AzureBlobDatastore\n",
    "from azure.ai.ml.entities import AccountKeyConfiguration\n",
    "\n",
    "store = AzureBlobDatastore(\n",
    "    name=\"blob_images_datastore\",\n",
    "    description=\"Blob Storage for images training data\",\n",
    "    account_name=\"<YOUR-STORAGE-ACCOUNT-NAME>\",\n",
    "    container_name=\"images-data\", \n",
    "    credentials=AccountKeyConfiguration(\n",
    "        account_key=\"<XXXX-XXXX>\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "ml_client.create_or_update(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "List the datastores again to verify that a new datastore named `blob_training_data` has been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1666790835295
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stores = ml_client.datastores.list()\n",
    "for ds_name in stores:\n",
    "    print(ds_name.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create data assets\n",
    "\n",
    "To point to a specific folder or file in a datastore, you can create data assets. There are three types of data assets:\n",
    "\n",
    "- `URI_FILE` points to a specific file.\n",
    "- `URI_FOLDER` points to a specific folder.\n",
    "- `MLTABLE` points to a MLTable file which specifies how to read one or more files within a folder.\n",
    "\n",
    "You'll now create URI_FOLDER data asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "To create a `URI_FOLDER` data asset, you have to specify a path that points to a specific folder. The path can be a local path or cloud path.\n",
    "\n",
    "In the example below, you'll create a data asset by referencing a *cloud* path. The path doesn't have to exist yet. The folder will be created when data is uploaded to the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1666793449117
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "blob_images_datastore_path = 'azureml://datastores/blob_images_datastore/paths/'\n",
    "\n",
    "my_data = Data(\n",
    "    path=blob_images_datastore_path,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"Data asset pointing to images data-asset-path folder in datastore\",\n",
    "    name=\"images-data-asset\"\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1666790894246
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = ml_client.data.list()\n",
    "for ds_name in datasets:\n",
    "    print(ds_name.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Use data in a job\n",
    "\n",
    "After using a notebook for experimentation. You can use scripts to train machine learning models. A script can be run as a job, and for each job you can specify inputs and outputs. \n",
    "\n",
    "You can use either **data assets** or **datastore paths** as inputs or outputs of a job. \n",
    "\n",
    "The cells below creates the **move-data.py** script in the **src** folder. The script reads the input data with the `read_csv()` function. The script then stores the data as a CSV file in the output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create a folder for the script files\n",
    "script_folder = 'src'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $script_folder/read_imagaes_data.py\n",
    "# import libraries\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def main(args):    \n",
    "    print(f\"analyzig data asset: {args.images_data_asset}\")\n",
    "    \n",
    "    # List all files and directories in the specified path\n",
    "    contents = os.listdir(args.images_data_asset)\n",
    "\n",
    "    # Print the contents\n",
    "    for item in contents:\n",
    "        print(item)\n",
    "    \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"--images_data_asset\", dest='images_data_asset',\n",
    "                        type=str)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "    \n",
    "    print(f\"args: {args}\")\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit a job that runs the **read_imagaes_data.py** script, run the cell below. \n",
    "\n",
    "The job is configured to use the data asset `images-data-asset`, pointing to the local **images-data** container as input. The output is a path pointing to a folder in the new datastore `blob_training_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InputOutputModes.RO_MOUNT - Read-only mount on the compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1666794414231
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml import command\n",
    "\n",
    "# ==============================================================\n",
    "# Set the mode. The popular modes include:\n",
    "# mode = InputOutputModes.RO_MOUNT # Read-only mount on the compute target\n",
    "# mode = InputOutputModes.DOWNLOAD # Download the data to the compute target\n",
    "# ==============================================================\n",
    "mode = InputOutputModes.RO_MOUNT # Read-only mount on the compute target\n",
    "# mode = InputOutputModes.DOWNLOAD # Download the data to the compute target\n",
    "\n",
    "\n",
    "# configure input and output\n",
    "my_job_inputs = {\n",
    "    \"images_data_asset\": Input(type=AssetTypes.URI_FOLDER,\n",
    "                            path=\"images-data-asset:1\",\n",
    "                            mode=mode)\n",
    "}\n",
    "\n",
    "# configure job\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python read_imagaes_data.py --images_data_asset ${{inputs.images_data_asset}}\",\n",
    "    inputs=my_job_inputs,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
    "    compute=\"cpu-cluster-DS3v2\",\n",
    "    display_name=\"explore-images-data-RO_MOUNT\",\n",
    "    experiment_name=\"explore-images-data\"\n",
    ")\n",
    "\n",
    "# submit job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "aml_url = returned_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InputOutputModes.DOWNLOAD - Download the data to the compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml import command\n",
    "\n",
    "# ==============================================================\n",
    "# Set the mode. The popular modes include:\n",
    "# mode = InputOutputModes.RO_MOUNT # Read-only mount on the compute target\n",
    "# mode = InputOutputModes.DOWNLOAD # Download the data to the compute target\n",
    "# ==============================================================\n",
    "# mode = InputOutputModes.RO_MOUNT # Read-only mount on the compute target\n",
    "mode = InputOutputModes.DOWNLOAD # Download the data to the compute target\n",
    "\n",
    "\n",
    "# configure input and output\n",
    "my_job_inputs = {\n",
    "    \"images_data_asset\": Input(type=AssetTypes.URI_FOLDER,\n",
    "                            path=\"images-data-asset:1\",\n",
    "                            mode=mode)\n",
    "}\n",
    "\n",
    "# configure job\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python read_imagaes_data.py --images_data_asset ${{inputs.images_data_asset}}\",\n",
    "    inputs=my_job_inputs,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
    "    compute=\"cpu-cluster-DS3v2\",\n",
    "    display_name=\"explore-images-data-DOWNLOAD\",\n",
    "    experiment_name=\"explore-images-data\"\n",
    ")\n",
    "\n",
    "# submit job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "aml_url = returned_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2b2cd046deda8eabef1e765a11d0ec9aa9bd1d31d56ce79c815a38c323e14ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
